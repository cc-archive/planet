import sys
import re
import pickle
import urllib
from xml.dom import minidom
from BeautifulSoup import BeautifulSoup
import planet

##### ------------------- User Configurable Options ---------------------- #####

# URI for the CC API to which we'll append each license URI in order to get
# back specific data regarding the license.
cc_api = 'http://api.creativecommons.org/rest/dev/details?license-uri='

# The name of the directory where CC related cache file live, relative to the
# cache directory configured for Planet Venus
cc_cache_dir = "creativecommons"

# The name of the file where CC license URI -> description mappings lives
map_file = "cc_license_map"

##### -------------------------------------------------------------------- #####


def get_cc_license_text(href):
    """This function will try to fetch data about the passed CC license URL.
    It uses the CC API."""

    try:
        # Try to fetch some data about this CC license
        data = urllib.urlopen(cc_api + href)
    except IOError:
        # If we can't get ahold of cc_api, then abandon this process
        return None
    else:
        # Parse data returned from cc_api and extract the name of the license.
        # Later on, more info could be extracted from this if desired.
        xmldoc = minidom.parse(data)
        # Only proceed if the license name atually exists
        try:
            text = xmldoc.getElementsByTagName("license-name")[0] \
                .childNodes[0].toxml()
        except IndexError:
            # If the data isn't what we expected then abandon this process
            return None
        else:
            return text

def main():
    # Set up the logging object
    log = planet.logger

    # Locate the cache directory and look for the file where a serialized list of
    # URI->description mappings may or may not exist
    planet_cache_dir= config.cache_directory()
    cc_cache_path = os.path.join(planet_cache_dir, 'creativecommons')
    if not os.path.exists(cc_cache_path):
        os.makedirs(cc_cache_path)
    map_path = os.path.join(cc_cache_path, 'license_mappings')

    # If cc_license_cache is not empty, it should contain a dict of
    # URI->description mappings, else just create an empty dict
    if os.path.isfile(map_path):
        fh = open(map_path, 'r')
        license_maps = pickle.load(fh)
        fh.close()
    else:
        license_maps = {}

    # Parse STDIN, which should be the HTML page generated by Planet Venus
    soup = BeautifulSoup(sys.stdin.read())

    # Find every anchor tag that has rel="license"
    anchors = soup.findAll(name="a", attrs={"rel": "license"})

    # Step through each anchor and if it's a Creative Commons license, then lookup
    # more specific data about the license that should be used to describe the
    # license.
    for anchor in anchors:
        # If the href URL begins with http://creativecommons.org then we'll take
        # this to be a Creative Commons license
    
        if re.match(r'^http:\/\/creativecommons\.org', anchor['href']):
            # First check to see if the URI has already been mapped to a
            # human-readable name.  If so, use the cached copy, if not, go look it
            # up
            if anchor['href'] in license_maps:
                lic_text = license_maps[anchor['href']]
            else:
                lic_text = get_cc_license_text(anchor['href'])
                # Re-set the value of this mapping, just in case it changed
                license_maps[anchor['href']] = lic_text
                # And now change the license verbiage
                anchor.contents = lic_text

    # Output the perhaps updated version of the HTML doc
    print soup.prettify()
    #sys.stdout.write(soup)

    # Now re-serialize the license mappings and write a new cache file
    fh = open(map_path, 'w')
    pickle.dump(license_maps, fh)
    fh.close()


if __name__ == '__main__':
    main()
