#!/usr/bin/env python

"""This plugin can be used to lookup and add extended license information to an
HTML document.

It accepts an (X)HTML document as input via STDIN and will search for any RDFa
marked-up anchor tags that specify license information - indicated by the
attribute rel="license".  It will then attempt to apply an appropriate name to
the license based on the URL (href attribute).  For example:

The URL http://creativecommons.org/licenses/by-sa/3.0/ indicates a Creative
Commons "Attribution-ShareAlike 3.0 Unported" license.  This may not be readily
apparently to everyone based solely on the URL, so this plugin will lookup the
approriate name and then insert it as the text of the anchor tag.

This plugin will work for any license URL that specifies something like:
<link rel="alternate" type="application/rdf+xml" href="rdf"/> somewhere at the
other end of the license URI.  It will then fetch rdf+xml document and parse
it, looking for an appropriate title.
"""

import sys
import os
import pickle
import urllib2
import urlparse
from xml.dom import minidom
import BeautifulSoup
import rdfadict
import planet

# Instantiate the planet logging object
log = planet.logger

# Set default license text.  That is, if this plugin fails to find a suitable
# name in the license URL -> license name map cache, or can't find it on the
# Internet, then it will insert this name
default_text = "License"

def get_license_text(license_url):
    """Try to fetch license name associcated with the passed URL."""
    log.debug('Entered function get_license_text()')
    try:
        # Try to fetch some data about this license
        log.debug('Trying to fetch URL ' + license_url)
        lic_triples = rdfadict.RdfaParser().parseurl(license_url)
    except IOError:
        # If we can't get fetch the document then abandon this process
        log.debug('Encountered an IOError while trying to fetch RDFa data')
        return None
    else:
        ns_title = 'http://purl.org/dc/elements/1.1/title'
        if ns_title in lic_triples[license_url]:
            text = lic_triples[license_url][ns_title][0]
            log.debug('Found license name ' + text)
            return text
        else:
            # Return something generic if no real license text is found
            log.debug('License name NOT found.')
            return None

def main():
    """Main function for parsing the HTML document."""
    # Locate the cache directory and look for the file where a serialized list
    # of URI->description mappings may or may not exist
    planet_cache_dir= planet.config.cache_directory()
    cc_cache_path = os.path.join(planet_cache_dir, 'creativecommons')
    log.debug('CC cache directory is ' + cc_cache_path)
    if not os.path.exists(cc_cache_path):
        log.debug('CC cache directory does not exist.  Trying to create it ...')
        os.makedirs(cc_cache_path)
    map_path = os.path.join(cc_cache_path, 'license_mappings')
    log.debug('CC URL -> license-name map cache file is ' + map_path)

    # If cc_license_cache is not empty, it should contain a dict of
    # URI -> license-name mappings, else just create an empty dict
    if os.path.isfile(map_path):
        log.debug('CC URI -> license-name map cache file does not exist.')
        fh = open(map_path, 'r')
        license_maps = pickle.load(fh)
        fh.close()
    else:
        log.debug('Creating a blank URI -> license-name map dict.')
        license_maps = {}

    # Parse STDIN, which should be the HTML page generated by Planet Venus
    log.debug('Attepting to parse the document sent via STDIN.')
    soup = BeautifulSoup.BeautifulSoup(sys.stdin.read())

    # Find every anchor tag that has rel="license"
    log.debug('Extracting license-related anchor tags from the parse tree.')
    anchors = soup.findAll(name="a", attrs={"rel": "license"})

    # Step through each anchor and if it's a Creative Commons license, then
    # lookup more specific data about the license that should be used to
    # describe the license.
    for anchor in anchors:
        log.debug('Iterating through found license-related anchor tags.')
    
        # First check to see if the URI has already been mapped to a
        # human-readable name.  If so, use the cached copy, if not, look it
        # up
        if anchor['href'] in license_maps:
            log.debug('URL found in cache file. Using cached license-name.')
            lic_text = license_maps[anchor['href']]
            anchor.contents[0].replaceWith(
                BeautifulSoup.NavigableString(lic_text))
        else:
            # Try to fetch the license text from the CC API
            log.debug('URL NOT found in cache file. Will attempt lookup.')
            lic_text =  get_license_text(anchor['href'])
            if lic_text:
                log.debug('License-name lookup succeded. Caching result.')
                # Re-set the value of this mapping, just in case it changed
                license_maps[anchor['href']] = lic_text
            else:
                lic_text = default_text

            # Change the text of the license anchor tag
            anchor.contents[0].replaceWith(
                BeautifulSoup.NavigableString(lic_text))

    # Output the perhaps updated version of the HTML doc
    log.debug('Writing document to STDOUT.')
    sys.stdout.write(soup.renderContents())

    # Now re-serialize the license mappings and write a new cache file
    log.debug('Serializing data.  Writing new URL -> license-name cache file.')
    fh = open(map_path, 'w')
    pickle.dump(license_maps, fh)
    fh.close()


if __name__ == '__main__':
    main()
