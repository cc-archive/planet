#!/usr/bin/env python

"""Planet Venus plugin to lookup and add extended Creative Commons license
information to an HTML document.

This script/plugin will accept an (X)HTML document as input via STDIN and will
search for any RDFa marked-up anchor tags that specify license information -
indicated by the attribute rel="license".  It will then attempt to apply an
appropriate name to the license based on the URL (href attribute).  For example:

The URL http://creativecommons.org/licenses/by-sa/3.0/ indicates a Creative
Commons "Attribution-ShareAlike 3.0 Unported" license.  This may not be readily
apparently to everyone based solely on the URL, so this plugin will lookup the
approriate name and then insert it as the text of the anchor tag.

If the license is not a CC license, then the plugin will ignore the anchor tag.

"""

import sys
import re
import pickle
import urllib
from xml.dom import minidom
import BeautifulSoup
import planet

# URI for the CC API to which we'll append each license URI in order to get
# back specific data regarding the license.  For more information see:
# http://api.creativecommons.org/docs/
cc_api = 'http://api.creativecommons.org/rest/dev/details?license-uri='

# Instantiate the planet logging object
log = planet.logger


def get_cc_license_text(href):
    """Try to fetch data about the passed CC license URL. It uses the CC API."""
    log.debug('Entered function get_cc_license_text()')
    try:
        # Try to fetch some data about this CC license
        log.debug('Trying to fetch URL ' + cc_api + href)
        data = urllib.urlopen(cc_api + href)
    except IOError:
        # If we can't get ahold of cc_api, then abandon this process
        log.debug('Encountered an IOError while trying to fetch URL.')
        return None
    else:
        # Parse data returned from cc_api and extract the name of the license.
        # A number of data elements are returned, but at the moment we are only
        # concerned with 'license-name'.  More info could be extracted from this
        # if desired.
        xmldoc = minidom.parse(data)
        try:
            log.debug('Trying to parse XML retuned from the the CC Web API.')
            text = xmldoc.getElementsByTagName("license-name")[0] \
                .childNodes[0].toxml()
        except IndexError:
            # If the data isn't what we expected then abandon this process
            log.debug('Encountered an IndexError while trying to to extract \
                license-name.')
            return None
        else:
            log.debug('Found license name ' + text)
            return text

def main():
    """Main function for parsing the HTML document."""
    # Locate the cache directory and look for the file where a serialized list
    # of URI->description mappings may or may not exist
    planet_cache_dir= config.cache_directory()
    cc_cache_path = os.path.join(planet_cache_dir, 'creativecommons')
    log.debug('CC cache directory is ' + cc_cache_path)
    if not os.path.exists(cc_cache_path):
        log.debug('CC cache directory does not exist.  Trying to create it ...')
        os.makedirs(cc_cache_path)
    map_path = os.path.join(cc_cache_path, 'license_mappings')
    log.debug('CC URL -> license-name map cache file is ' + map_path)

    # If cc_license_cache is not empty, it should contain a dict of
    # URI -> license-name mappings, else just create an empty dict
    if os.path.isfile(map_path):
        log.debug('CC URI -> license-name map cache file does not exist.')
        fh = open(map_path, 'r')
        license_maps = pickle.load(fh)
        fh.close()
    else:
        log.debug('Creating a blank URI -> license-name map dict.')
        license_maps = {}

    # Parse STDIN, which should be the HTML page generated by Planet Venus
    log.debug('Attepting to parse the document sent via STDIN.')
    soup = BeautifulSoup.BeautifulSoup(sys.stdin.read())

    # Find every anchor tag that has rel="license"
    log.debug('Extracting license-related anchor tags from the parse tree.')
    anchors = soup.findAll(name="a", attrs={"rel": "license"})

    # Step through each anchor and if it's a Creative Commons license, then
    # lookup more specific data about the license that should be used to
    # describe the license.
    for anchor in anchors:
        # If the href URL begins with http://creativecommons.org then we'll take
        # this to be a Creative Commons license
        log.debug('Iterating through found license-related anchor tags.')
    
        if re.match(r'^http:\/\/creativecommons\.org', anchor['href']):
            # First check to see if the URI has already been mapped to a
            # human-readable name.  If so, use the cached copy, if not, look it
            # up
            if anchor['href'] in license_maps:
                log.debug('URL found in cache file. Using cached license-name.')
                lic_text = license_maps[anchor['href']]
                anchor.contents[0].replaceWith(
                    BeautifulSoup.NavigableString(lic_text))
            else:
                # Try to fetch the license text from the CC API
                log.debug('URL NOT found in cache file. Will attempt lookup.')
                lic_text =  get_cc_license_text(anchor['href'])
                if lic_text:
                    log.debug('License-name lookup succeded. Caching result.')
                    # Change the license verbiage
                    anchor.contents[0].replaceWith(
                        BeautifulSoup.NavigableString(lic_text))
                    # Re-set the value of this mapping, just in case it changed
                    license_maps[anchor['href']] = lic_text

    # Output the perhaps updated version of the HTML doc
    log.debug('Writing document to STDOUT.')
    sys.stdout.write(soup.renderContents())

    # Now re-serialize the license mappings and write a new cache file
    log.debug('Serializing data.  Writing new URL -> license-name cache file.')
    fh = open(map_path, 'w')
    pickle.dump(license_maps, fh)
    fh.close()


if __name__ == '__main__':
    main()
