#!/usr/bin/env python

"""This plugin can be used to lookup and add extended license information to an
HTML document.

It accepts an (X)HTML document as input via STDIN and will search for any RDFa
marked-up anchor tags that specify license information - indicated by the
attribute rel="license".  It will then attempt to apply an appropriate name to
the license based on the URL (href attribute).  For example:

The URL http://creativecommons.org/licenses/by-sa/3.0/ indicates a Creative
Commons "Attribution-ShareAlike 3.0 Unported" license.  This may not be readily
apparently to everyone based solely on the URL, so this plugin will lookup the
approriate name and then insert it as the text of the anchor tag.

This plugin should work for any licensewill work for any license URL that specifies something like:
<link rel="alternate" type="application/rdf+xml" href="rdf"/> somewhere at the
other end of the license URI.  It will then fetch rdf+xml document and parse
it, looking for an appropriate title.
"""

import sys
import os
import pickle
import urllib
import urllib2
import urlparse
from xml.dom import minidom
import BeautifulSoup
import rdfadict
import planet

# Instantiate the planet logging object
log = planet.logger

# Set default license text and title.  That is, if this plugin fails to find a
# suitable name in the license name map cache, or can't find it on the
# Internet, then it will insert this name
default_text = "License"
default_title = "License information"

def get_license_text(license_url):
    """Try to fetch license name associcated with the passed URL."""
    log.debug('Entered function get_license_text()')

    # some blog plugins erroneously urlquote the license URL
    license_url = urllib.unquote(license_url)

    try:
        # Try to fetch some data about this license
        log.debug('Trying to fetch URL ' + license_url)
        lic_triples = rdfadict.RdfaParser().parseurl(license_url)
    except IOError:
        # If we can't get fetch the document then abandon this process
        log.debug('Encountered an IOError while trying to fetch RDFa data')
        return None
    else:
        if license_url in lic_triples:
            ns_title = 'http://purl.org/dc/elements/1.1/title'
            if ns_title in lic_triples[license_url]:
                text = lic_triples[license_url][ns_title][0]
                log.debug('Found license name ' + text)
                return text
        else:
            # Return something generic if no real license text is found
            log.debug('License name NOT found.')
            return None

def main():
    """Main function for parsing the HTML document."""
    # Locate the cache directory and look for the file where a serialized list
    # of URI->description mappings may or may not exist
    planet_cache_dir= planet.config.cache_directory()
    cache_path = os.path.join(planet_cache_dir, 'license_mappings')
    log.debug('Cache directory is ' + cache_path)
    if not os.path.exists(cache_path):
         log.debug('Cache directory does not exist.  Trying to create it ...')
         os.makedirs(cache_path)
    map_path = os.path.join(cache_path, 'license.maps')
    log.debug('License name map cache file is ' + map_path)

    # If cc_license_cache is not empty, it should contain a dict of
    # URI -> license-name mappings, else just create an empty dict
    if os.path.isfile(map_path):
        log.debug('License name map cache file found.  Loading mappings.')
        fh = open(map_path, 'r')
        license_maps = pickle.load(fh)
        fh.close()
    else:
        log.debug('License name map cache file not found. Creating empty dict.')
        license_maps = {}

    # Parse STDIN, which should be the HTML page generated by Planet Venus
    log.debug('Attepting to parse the document sent via STDIN.')
    soup = BeautifulSoup.BeautifulSoup(sys.stdin.read())

    # Find every anchor tag that has rel="license"
    log.debug('Extracting license-related anchor tags from the parse tree.')
    anchors = soup.findAll(name="a", attrs={"rel": "license"})
    for anchor in anchors:
        log.debug('Processing license URL ' + anchor['href'])

        # First check to make sure that this is THE license URL
        # and not just some license URL that might happen to be
        # in the HTML content somewhere.  This is determined by
        # the id of the enclosing <span> being "license_url"
        if not anchor.parent.has_key("id"):
            continue
        else:
            if not anchor.parent["id"] == "license_url":
                continue
        
        # Check to see if the URI has already been mapped to a
        # human-readable name.  If so, use the cached copy, if not, look it
        # up
        if anchor['href'] in license_maps:
            lic_text = license_maps[anchor['href']]
            lic_title = lic_text
            log.debug('Found name in cache: ' + lic_text)
        else:
            # Try to fetch the license name from the license URL
            log.debug('URL NOT found in cache.  Attempting lookup.')
            lic_text =  get_license_text(anchor['href'])
            if lic_text:
                # Cache the license name
                license_maps[anchor['href']] = lic_text
                log.debug('License name lookup succeded. Cached result.')
                lic_title = lic_text
            else:
                lic_text = default_text
                lic_title = default_title
                log.debug('Using default license name ' + lic_text)

        # Change the text and title of the license anchor tag
        anchor.contents[0].replaceWith(
            BeautifulSoup.NavigableString(lic_text))
        anchor['title'] = lic_title

    # Output the perhaps updated version of the HTML doc
    log.debug('Writing document to STDOUT.')
    sys.stdout.write(soup.renderContents())

    # Now re-serialize the license mappings and write a new cache file
    log.debug('Serializing data.  Writing new license name map cache file.')
    fh = open(map_path, 'w')
    pickle.dump(license_maps, fh)
    fh.close()


if __name__ == '__main__':
    main()
